{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njm8ixFOP38b"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISRVFje6QymB"
   },
   "outputs": [],
   "source": [
    "# Read the Dataset\n",
    "dataset = pd.read_csv(\"/usr/local/notebooks/datasets/iris.csv\")\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "dataset[\"species\"]= label_encoder.fit_transform(dataset[\"species\"]) # Encoding Output Variable\n",
    "# Separating the Input and Output Columns\n",
    "X = dataset[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "y = dataset[\"species\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxQeg2gnRWgk"
   },
   "source": [
    "In the above code, we load the dataset and then encode the target variable. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvomUdPjfHqt"
   },
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.02\n",
    "params['objective'] = 'multiclass'\n",
    "params['num_class'] = '3'\n",
    "params['metric'] = 'multiclass'\n",
    "params['max_depth'] = 10\n",
    "clf = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhotSNZSgKkS"
   },
   "source": [
    "[1] : We specify the learning rate to be 0.003. <br>\n",
    "[2] : We specify the objective to be \"multiclass\", as it is meant for Multi-Class Classification Problems. <br>\n",
    "[3] : Same reason goes for \"metric\". It is a version of Log Loss for Multi-Class classification. <br>\n",
    "[4] : We specify the \"num_class\" to be 3, because here we are having 3 classes. <br>\n",
    "[5] : We specify the \"max_depth\" to be 10 as it is a small dataset and we don't want to overfit. <br>\n",
    "[6] : The value 100 passed in the train function, is the number of boosting iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqCjehJCQyrX"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "OQ7snzyzjBAL",
    "outputId": "306d732f-0cdf-451a-f600-4ff17a62c9c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06352708, 0.06405324, 0.87241968],\n",
       "       [0.07103118, 0.85116183, 0.07780699],\n",
       "       [0.88880191, 0.05146376, 0.05973433],\n",
       "       [0.05984664, 0.06034586, 0.87980751],\n",
       "       [0.88681768, 0.05134887, 0.06183345],\n",
       "       [0.05241377, 0.05283703, 0.8947492 ],\n",
       "       [0.8868256 , 0.05134933, 0.06182507],\n",
       "       [0.06209866, 0.74320358, 0.19469776],\n",
       "       [0.06682658, 0.79964298, 0.13353044],\n",
       "       [0.07103728, 0.85123487, 0.07772785]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc1y6Hv-C1_R"
   },
   "source": [
    "1. \"y_pred\" contains the probability of belonging to each class for each instance in the test dataset. We will convert these probabilities back to classes as below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BOIWF04CjpNC",
    "outputId": "86527550-44b8-4ab3-952d-82985c5a6fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test set is 0.36\n"
     ]
    }
   ],
   "source": [
    " import numpy as np\n",
    " from sklearn.metrics import accuracy_score\n",
    " y_pred = [np.argmax(line) for line in y_pred]\n",
    "print(\"The accuracy on test set is {0:.2f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7DZABKajqVn"
   },
   "source": [
    "1. On Line 3 we take the probabilities produced for each instance and generate the class label based on the Maximum Probability assigned. <br>\n",
    "2. On Line 4, we calculate the accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LightGBM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
